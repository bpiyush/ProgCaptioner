{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc2ecf1-afcc-45b8-aa08-78578571c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=\"/athenahomes/piyush/projects/ProgCaptioner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159086e9-ea79-4479-aff2-5b9edca540dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import torchvideotransforms. Proceeding without.\n",
      "Please install using:\n",
      "pip install git+https://github.com/hassony2/torch_videovision\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/athenahomes/piyush/projects/ProgCaptioner\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "import shared.utils as su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a47cade-c5c9-4829-997f-cb7fa051753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: str) -> dict:\n",
    "    \"\"\"Helper to load json file\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7073c374-3110-4f2d-aa2b-61796d4dbe18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load json file\n",
    "out_dir = \"/scratch/shared/beegfs/piyush/datasets/Kinetics400/progress_captions\"\n",
    "json_file = os.path.join(out_dir, \"output_data.json\")\n",
    "data = load_json(json_file)\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a87598a9-fa47-47a4-ae28-6318c605e5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 'val_split/faceplanting/petld-72OXM_000001_000011',\n",
       " 'n_frames': 6,\n",
       " 'image_files': ['/scratch/shared/beegfs/piyush/datasets/Kinetics400/progress_captions/frames/val_split/faceplanting/petld-72OXM_000001_000011/frame000.png',\n",
       "  '/scratch/shared/beegfs/piyush/datasets/Kinetics400/progress_captions/frames/val_split/faceplanting/petld-72OXM_000001_000011/frame032.png',\n",
       "  '/scratch/shared/beegfs/piyush/datasets/Kinetics400/progress_captions/frames/val_split/faceplanting/petld-72OXM_000001_000011/frame065.png',\n",
       "  '/scratch/shared/beegfs/piyush/datasets/Kinetics400/progress_captions/frames/val_split/faceplanting/petld-72OXM_000001_000011/frame097.png',\n",
       "  '/scratch/shared/beegfs/piyush/datasets/Kinetics400/progress_captions/frames/val_split/faceplanting/petld-72OXM_000001_000011/frame130.png',\n",
       "  '/scratch/shared/beegfs/piyush/datasets/Kinetics400/progress_captions/frames/val_split/faceplanting/petld-72OXM_000001_000011/frame162.png'],\n",
       " 'query0': \"These are 6 frames extracted from a video sequence depicting faceplanting, provide a description for each frame.\\nRequirement: (1) Ensure each frame's description is specific to the corresponding frame, not referencing to other frames; (2) The description should focus on the specific action being performed, capturing the action and progression of the action. There is no need to comment on other elements, such as the background or unrelated objects.\\nReply with the following format:\\n<Frame 1>: Your description\\n...\\n<Frame 6>: Your description\\n\",\n",
       " 'response0': \"<Frame 1>: The video begins with a wide shot of a dirt path surrounded by dense green foliage. Two individuals are visible in the background, one wearing a blue shirt and the other in a white shirt. The scene is set in a lush, green environment with tall grass and various plants.\\n<Frame 2>: The focus shifts to a close-up of a person riding a motorcycle on the same dirt path. The rider is wearing a blue helmet and appears to be moving at speed, as indicated by the motion blur. The background consists of dense greenery, including bushes and trees, which suggests that the location is a forest or rural area.\\n<Frame 3>: The scene transitions to a closer view of the same individual, now clearly visible and actively riding the bicycle. The person is dressed in a blue helmet, gray shirt, and dark pants, and is navigating through a section of the trail characterized by uneven terrain and scattered debris. The surrounding environment remains lush and green, emphasizing the outdoor setting.\\n<Frame 4>: In this frame, the person is still riding the bicycle, but they have moved slightly forward compared to the previous frame. The background remains consistent with dense green foliage, and the person's position suggests they are continuing their ride along the same path. The focus is on the person's movement and the surrounding natural environment.\\n<Frame 5>: In this frame, the person is seen standing next to the bicycle, which is now on its side. The individual appears to be adjusting or inspecting the bike, possibly after a fall or to prepare for another attempt. The surrounding environment remains consistent with dense greenery and a dirt path.\\n<Frame 6>: The same person is now actively riding the bicycle down the same dirt path. They are wearing a gray long-sleeve shirt, light blue jeans, and a blue cap. The background remains consistent with dense green foliage and some fallen branches on the ground.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8de1f750-2f8a-4556-bec3-096e0a625692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flipping pancake\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0241c8d1bf46378f8e24415771c010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01\\x96\\x00\\x00\\x02\\xd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load images\n",
    "item = data[np.random.randint(len(data))]\n",
    "images = item[\"image_files\"]\n",
    "images = [PIL.Image.open(f) for f in images]\n",
    "captions = item[\"response0\"].split(\"\\n\")\n",
    "\n",
    "print(item[\"idx\"].split(\"/\")[1])\n",
    "# su.visualize.show_grid_of_images(images, n_cols=len(images), figsize=(14, 4), subtitles=captions)\n",
    "# su.visualize.display_frames_with_captions(images, captions, max_width=1200)\n",
    "\n",
    "su.visualize.display_frames_vertical_with_captions(images, captions, max_width=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47927859-76b0-45f7-bbf3-bb9bb597c40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24711bd7550e4aeab949a52e807bd896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01\\xe0\\x00\\x00\\x01h\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "su.visualize.display_frames_vertical_with_captions(images, captions, max_width=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea84330-d350-4937-9e7c-c4a53bc75435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8cabe-6e3a-49f7-96ac-afdf8ee23ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640ab4d-21cf-4f39-9105-2bae277d9ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7739c5-3d8a-4649-8908-dce06921fa26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2704d97-b822-4946-8ea2-7e9334d2e6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
